{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b0XwE5j0kwF1"
   },
   "source": [
    "## Query Output Preprocessing\n",
    "\n",
    "The graph database Dgraph returns query results in JSON format. The queries consist of getting all `originated` and all `responded` connections of a specified host. The `query_handler` tool converts these JSON outputs to CSV files (two CSV files for each host with some IP address - one for each connection direction (`originated`, `responded`)). \n",
    "\n",
    "This Jupyter notebook is used to:\n",
    "\n",
    "1. Compute the neighbourhoods of these hosts. *(For each connection, compute its neighbourhood which is given by connections in a given time interval.)*\n",
    "2. Concat DataFrames to one final DataFrames.\n",
    "3. Assign labels. \n",
    "4. Write the result to a single file, ready for ML preprocessing (data preparation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b0XwE5j0kwF1"
   },
   "source": [
    "## Neighbourhood Computation\n",
    "\n",
    "### 0. Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sramkova/diploma_thesis_data/cicids2017/attacks/3_friday/ddos/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "PREFIX_PATH = '/home/sramkova/diploma_thesis_data/cicids2017/attacks'\n",
    "\n",
    "# get last two dictinary names of current directory, they correspond to directory names of input data\n",
    "attack_dir_path = '/'.join(os.getcwd().split('/')[-2:])\n",
    "PREFIX = PREFIX_PATH + '/' + attack_dir_path + '/'\n",
    "print(PREFIX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XtBLVeiRhsp-",
    "outputId": "965b53d2-df1f-40af-e21d-97728113baaa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "1136\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "DIR_PATH_ORIG = PREFIX + 'originated'\n",
    "DIR_PATH_RESP = PREFIX + 'responded'\n",
    "\n",
    "\n",
    "file_list_orig = []\n",
    "file_list_resp = []\n",
    "\n",
    "def get_file_names(file_list, dir_path):\n",
    "    for filename in os.listdir(dir_path):\n",
    "        # only IPv4: \n",
    "        if 'f' not in filename and filename.endswith('.csv'):\n",
    "            # (if there is an 'f' present in the name of the file, it means that the file contains \n",
    "            # connections of a host with IPv6 address)\n",
    "            file_list.append(filename)\n",
    "\n",
    "# load filenames to lists:\n",
    "get_file_names(file_list_orig, DIR_PATH_ORIG)\n",
    "get_file_names(file_list_resp, DIR_PATH_RESP)\n",
    "\n",
    "print(len(file_list_orig))\n",
    "print(len(file_list_resp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "1136\n"
     ]
    }
   ],
   "source": [
    "# load as dataframes to a dictionary for easier processing:\n",
    "\n",
    "# elements of the dictionary are in a form: { host.ip -> df with connections of corresponding host }\n",
    "dfs_orig = {}\n",
    "dfs_resp = {}\n",
    "\n",
    "def load_files_to_dfs(dfs_dict, file_list, dir_path, prefix):\n",
    "    prefix_name = 'output-' + prefix\n",
    "    for filename in file_list:\n",
    "        file_ip = filename\n",
    "        file_ip = file_ip.replace(prefix_name, '').replace('.csv', '')\n",
    "        df_conns = pd.read_csv(dir_path + '/' + filename)\n",
    "\n",
    "        df_conns['connection.time'] = pd.to_datetime(df_conns['connection.ts'])\n",
    "        \n",
    "        # missing connection.service value means that Zeek wasn't able to extract the service => nulls can \n",
    "        # be treated as a new category\n",
    "        df_conns['connection.service'].fillna('none', inplace = True)\n",
    "\n",
    "        dfs_dict[file_ip] = df_conns\n",
    "\n",
    "load_files_to_dfs(dfs_orig, file_list_orig, DIR_PATH_ORIG, 'o-')\n",
    "load_files_to_dfs(dfs_resp, file_list_resp, DIR_PATH_RESP, 'r-')\n",
    "\n",
    "print(len(dfs_orig))\n",
    "print(len(dfs_resp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-07-07 18:55:00.685029+00:00\n",
      "2017-07-07 19:24:56.015158+00:00\n"
     ]
    }
   ],
   "source": [
    "# max, min times to check if they correspond to available attack times (considering the time shift):\n",
    "\n",
    "o_max = dfs_orig['192.168.10.25']['connection.time'][0]\n",
    "o_min = dfs_orig['192.168.10.25']['connection.time'][0]\n",
    "\n",
    "for o_ip in dfs_orig:\n",
    "    o_df = dfs_orig[o_ip]\n",
    "    cur_max = o_df['connection.time'].max()\n",
    "    cur_min = o_df['connection.time'].min()\n",
    "    if cur_max > o_max:\n",
    "        o_max = cur_max\n",
    "        # print(o_ip)\n",
    "    if cur_min < o_min:\n",
    "        o_min = cur_min\n",
    "        # print(o_ip)\n",
    "\n",
    "print(o_min)\n",
    "print(o_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Compute neighbourhoods for each row based on a time interval\n",
    "\n",
    "(e.g. time interval: +- 5 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# various stat functions on attributes from neighbourhood:\n",
    "\n",
    "def get_counts(df, prefix):\n",
    "    # counts (overall + counts of different protocols): \n",
    "    proto_tcp_count = 0\n",
    "    proto_udp_count = 0\n",
    "    proto_icmp_count = 0\n",
    "            \n",
    "    if 'connection.proto' in df:\n",
    "        proto_counts = df['connection.proto'].value_counts()\n",
    "        proto_tcp_count = proto_counts['tcp'] if 'tcp' in proto_counts else 0\n",
    "        proto_udp_count = proto_counts['udp'] if 'udp' in proto_counts else 0\n",
    "        proto_icmp_count = proto_counts['icmp'] if 'icmp' in proto_counts else 0\n",
    "    \n",
    "    return {prefix + '_total': len(df.index),\n",
    "            prefix + '_proto_tcp_count': proto_tcp_count,\n",
    "            prefix + '_proto_udp_count': proto_udp_count,\n",
    "            prefix + '_proto_icmp_count': proto_icmp_count\n",
    "           }\n",
    "\n",
    "def get_modes(df, prefix):\n",
    "    # .mode()[0] return the value of a categorical variable that appeared the most times\n",
    "    return {prefix + '_connection.protocol_mode': df['connection.proto'].mode()[0] if 'connection.proto' in df else '-',\n",
    "            prefix + '_connection.service_mode': df['connection.service'].mode()[0] if 'connection.service' in df else '-',\n",
    "            prefix + '_connection.conn_state_mode': df['connection.conn_state'].mode()[0] if 'connection.conn_state' in df else '-'\n",
    "           }\n",
    "\n",
    "def get_means(df, prefix):\n",
    "    # .mean() returns mean of the corresponding numerical attribute variable values\n",
    "    return {prefix + '_connection.time_mean': df['connection.time'].mean() if 'connection.time' in df else cur_time,\n",
    "            prefix + '_connection.duration_mean': df['connection.duration'].mean() if 'connection.duration' in df else 0, \n",
    "            # prefix + '_connection.orig_p_mean': df['connection.orig_p'].mean() if 'connection.orig_p' in df else 0, \n",
    "            prefix + '_connection.orig_bytes_mean': df['connection.orig_bytes'].mean() if 'connection.orig_bytes' in df else 0,\n",
    "            prefix + '_connection.orig_pkts_mean': df['connection.orig_pkts'].mean() if 'connection.orig_pkts' in df else 0, \n",
    "            # prefix + '_connection.resp_p_mean': df['connection.resp_p'].mean() if 'connection.resp_p' in df else 0,\n",
    "            prefix + '_connection.resp_bytes_mean': df['connection.resp_bytes'].mean() if 'connection.resp_bytes' in df else 0,\n",
    "            prefix + '_connection.resp_pkts_mean': df['connection.resp_pkts'].mean() if 'connection.resp_pkts' in df else 0\n",
    "           }\n",
    "\n",
    "def get_stats_means(df, prefix):\n",
    "    # .mean() returns mean of the corresponding numerical attribute variable values\n",
    "    return {prefix + '_dns_count_mean': df['dns_count'].mean() if 'dns_count' in df else 0,\n",
    "            prefix + '_ssh_count_mean': df['ssh_count'].mean() if 'ssh_count' in df else 0, \n",
    "            prefix + '_http_count_mean': df['http_count'].mean() if 'http_count' in df else 0,\n",
    "            prefix + '_ssl_count_mean': df['ssl_count'].mean() if 'ssl_count' in df else 0,\n",
    "            prefix + '_files_count_mean': df['files_count'].mean() if 'files_count' in df else 0\n",
    "           }\n",
    "\n",
    "def get_medians(df, prefix):\n",
    "    # .median() returns median of the corresponding numerical attribute variable values\n",
    "    return {prefix + '_connection.time_median': df['connection.time'].median() if 'connection.time' in df else cur_time,\n",
    "            prefix + '_connection.duration_median': df['connection.duration'].median() if 'connection.duration' in df else 0, \n",
    "            # prefix + '_connection.orig_p_median': df['connection.orig_p'].median() if 'connection.orig_p' in df else 0,\n",
    "            prefix + '_connection.orig_bytes_median': df['connection.orig_bytes'].median() if 'connection.orig_bytes' in df else 0,\n",
    "            prefix + '_connection.orig_pkts_median': df['connection.orig_pkts'].median() if 'connection.orig_pkts' in df else 0, \n",
    "            # prefix + '_connection.resp_p_median': df['connection.resp_p'].median() if 'connection.resp_p' in df else 0,\n",
    "            prefix + '_connection.resp_bytes_median': df['connection.resp_bytes'].median() if 'connection.resp_bytes' in df else 0,\n",
    "            prefix + '_connection.resp_pkts_median': df['connection.resp_pkts'].median() if 'connection.resp_pkts' in df else 0\n",
    "           }\n",
    "\n",
    "def get_orig_ports(df, prefix):\n",
    "    # count orig_p categories:\n",
    "    orig_well_known_count = 0\n",
    "    orig_reg_or_dyn_count = 0\n",
    "    unique_orig_p_list = df['connection.orig_p'].unique().tolist()\n",
    "    values_orig_p = df['connection.orig_p'].value_counts()\n",
    "    \n",
    "    for uniq_p in unique_orig_p_list:\n",
    "        if uniq_p < 1024:\n",
    "            orig_well_known_count += values_orig_p[uniq_p]\n",
    "        else:\n",
    "            orig_reg_or_dyn_count += values_orig_p[uniq_p]\n",
    "            \n",
    "    return {prefix + '_orig_p_well_known_count': orig_well_known_count,\n",
    "            prefix + '_orig_p_reg_or_dyn_count': orig_reg_or_dyn_count}\n",
    "\n",
    "def get_resp_ports(df, prefix):\n",
    "    # count resp_p categories:\n",
    "    common_ports = {21: 0, \n",
    "                    22: 0, \n",
    "                    53: 0, \n",
    "                    80: 0, \n",
    "                    123: 0, \n",
    "                    443: 0, \n",
    "                    3389: 0}\n",
    "    resp_well_known = 0\n",
    "    resp_reg = 0\n",
    "    resp_dyn = 0\n",
    "    unique_resp_p_list = df['connection.resp_p'].unique().tolist()\n",
    "    values_resp_p = df['connection.resp_p'].value_counts()\n",
    "    \n",
    "    for uniq_p in unique_resp_p_list:\n",
    "        if uniq_p in common_ports.keys():\n",
    "            common_ports[uniq_p] += values_resp_p[uniq_p]\n",
    "        elif uniq_p < 1024:\n",
    "            resp_well_known += values_resp_p[uniq_p]\n",
    "        elif uniq_p < 49152:\n",
    "            resp_reg += values_resp_p[uniq_p]\n",
    "        else:\n",
    "            resp_dyn += values_resp_p[uniq_p]\n",
    "            \n",
    "    return {prefix + '_resp_p_21_count': common_ports[21],\n",
    "            prefix + '_resp_p_22_count': common_ports[22],\n",
    "            prefix + '_resp_p_53_count': common_ports[53], \n",
    "            prefix + '_resp_p_80_count': common_ports[80],\n",
    "            prefix + '_resp_p_123_count': common_ports[123],\n",
    "            prefix + '_resp_p_443_count': common_ports[443],\n",
    "            prefix + '_resp_p_3389_count': common_ports[3389],\n",
    "            prefix + '_resp_p_well_known_count': resp_well_known,\n",
    "            prefix + '_resp_p_reg_count': resp_reg,\n",
    "            prefix + '_resp_p_dyn_count': resp_dyn}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_duration_filter(duration_val):\n",
    "    # based on constants from data_exploration.ipynb\n",
    "    if duration_val <= 0.0:\n",
    "        return 0.000001, None\n",
    "    elif duration_val <= 0.0001:\n",
    "        return 0.000001, 0.001\n",
    "    elif duration_val <= 0.009:\n",
    "        return 0.001, 0.05\n",
    "    elif duration_val <= 0.5:\n",
    "        return 0.05, 1.5\n",
    "    elif duration_val <= 5:\n",
    "        return 1.5, 10\n",
    "    elif duration_val <= 15:\n",
    "        return 10, 20\n",
    "    elif duration_val <= 30:\n",
    "        return 20, 40\n",
    "    elif duration_val <= 50:\n",
    "        return 40, 60\n",
    "    elif duration_val <= 75:\n",
    "        return 60, 90\n",
    "    elif duration_val <= 100:\n",
    "        return 75, 110\n",
    "    return None, 100\n",
    "\n",
    "def generate_bytes_filter(bytes_val):\n",
    "    if bytes_val == 0:\n",
    "        return 0, 0\n",
    "    elif bytes_val <= 1450:\n",
    "        return bytes_val - 50, bytes_val + 50\n",
    "    elif bytes_val <= 35000:\n",
    "        return bytes_val - 500, bytes_val + 500\n",
    "    else:\n",
    "        return None, bytes_val - 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similar_count(df, row, prefix):\n",
    "    # protocol filter\n",
    "    mask = (df['connection.proto'] == row['connection.proto'])\n",
    "    df_filtered = df.loc[mask]\n",
    "    \n",
    "    # service filter\n",
    "    mask = (df_filtered['connection.service'] == row['connection.service'])\n",
    "    df_filtered = df_filtered.loc[mask]\n",
    "    \n",
    "    # conn_state filter\n",
    "    mask = (df_filtered['connection.conn_state'] == row['connection.conn_state'])\n",
    "    df_filtered = df_filtered.loc[mask]\n",
    "    \n",
    "    # duration filter\n",
    "    lower, upper = generate_duration_filter(row['connection.duration'])\n",
    "    if lower:\n",
    "        mask = df_filtered['connection.duration'] >= lower\n",
    "        df_filtered = df_filtered.loc[mask]\n",
    "    if upper:\n",
    "        mask = df_filtered['connection.duration'] <= upper\n",
    "        df_filtered = df_filtered.loc[mask]\n",
    "        \n",
    "    # _bytes filter\n",
    "    lower, upper = generate_duration_filter(row['connection.orig_bytes'])\n",
    "    if lower:\n",
    "        mask = df_filtered['connection.orig_bytes'] >= lower\n",
    "        df_filtered = df_filtered.loc[mask]\n",
    "    if upper:\n",
    "        mask = df_filtered['connection.orig_bytes'] <= upper\n",
    "        df_filtered = df_filtered.loc[mask]\n",
    "        \n",
    "    lower, upper = generate_duration_filter(row['connection.resp_bytes'])\n",
    "    if lower:\n",
    "        mask = df_filtered['connection.resp_bytes'] >= lower\n",
    "        df_filtered = df_filtered.loc[mask]\n",
    "    if upper:\n",
    "        mask = df_filtered['connection.resp_bytes'] <= upper\n",
    "        df_filtered = df_filtered.loc[mask]\n",
    "    \n",
    "    # _ip_bytes filter\n",
    "    mask = (df_filtered['connection.orig_ip_bytes'] >= row['connection.orig_ip_bytes'] - 50) & (df_filtered['connection.orig_ip_bytes'] <= row['connection.orig_ip_bytes'] + 50)\n",
    "    df_filtered = df_filtered.loc[mask]\n",
    "    mask = (df_filtered['connection.resp_ip_bytes'] >= row['connection.resp_ip_bytes'] - 50) & (df_filtered['connection.resp_ip_bytes'] <= row['connection.resp_ip_bytes'] + 50)\n",
    "    df_filtered = df_filtered.loc[mask]\n",
    "    \n",
    "    # remove original connection from neighbourhood (empty will have size 0 instead of 1)\n",
    "    mask = (df_filtered['connection.uid'] != row['connection.uid'])\n",
    "    df_filtered = df_filtered.loc[mask]\n",
    "\n",
    "    return {prefix + '_similar_conns_count': df_filtered.shape[0]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_attr_value(x, attr_str, row_attr_vals_list):\n",
    "    if isinstance(x, float) and np.isnan(x):\n",
    "        return False\n",
    "    \n",
    "    if isinstance(x, list) and len(x) < 1:\n",
    "        return False\n",
    "    \n",
    "    if isinstance(x, str) and x == '[]':\n",
    "        return False\n",
    "    \n",
    "    if isinstance(row_attr_vals_list, list) and len(row_attr_vals_list) > 0:\n",
    "        for attribute in x:\n",
    "            if attribute in row_attr_vals_list:\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def get_similar_attributes_count(df, row, prefix):\n",
    "    neighbourhood_attributes_dict = {}\n",
    "    attributes = ['dns_qtype', 'dns_rcode', 'ssh_auth_attempts', 'ssh_host_key', 'http_method', 'http_status_code', \n",
    "                  'http_user_agent', 'ssl_version', 'ssl_cipher', 'ssl_curve', 'ssl_validation_status', 'files_source',\n",
    "                  'file_md5']\n",
    "    \n",
    "    for attr in attributes:\n",
    "        if not row[attr]:\n",
    "            # attribute value list is empty, no similarity is counted\n",
    "            attr_dict = {prefix + '_similar_' + attr + '_count': 0}\n",
    "            neighbourhood_attributes_dict.update(attr_dict)\n",
    "        else:\n",
    "            # filter\n",
    "            mask = df[attr].apply(lambda x: check_attr_value(x, attr, row[attr]))\n",
    "            df_filtered = df.loc[mask]\n",
    "\n",
    "            # remove original connection from neighbourhood (empty will have size 0 instead of 1)\n",
    "            mask = (df_filtered['connection.uid'] != row['connection.uid'])\n",
    "            df_filtered = df_filtered.loc[mask]\n",
    "\n",
    "            # add attribute count to dictionary that contains all counts\n",
    "            attr_dict = {prefix + '_similar_' + attr + '_count': df_filtered.shape[0]}\n",
    "            neighbourhood_attributes_dict.update(attr_dict)\n",
    "    \n",
    "    return neighbourhood_attributes_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_time_neighbourhood(host_ip, dfs_list, time_col_name, cur_time, time_start, time_end, row, prefix):\n",
    "    if host_ip in dfs_list:\n",
    "        ip_df = dfs_list[host_ip]\n",
    "        mask = (ip_df[time_col_name] > time_start) & (ip_df[time_col_name] <= time_end)\n",
    "        df = ip_df.loc[mask]\n",
    "\n",
    "        if len(df) > 0:\n",
    "            neighbourhood_dict = {}\n",
    "\n",
    "            neighbourhood_counts = get_counts(df, prefix)\n",
    "            neighbourhood_modes = get_modes(df, prefix)\n",
    "            neighbourhood_means = get_means(df, prefix)\n",
    "            # neighbourhood_medians = get_medians(df, prefix)\n",
    "            neighbourhood_orig_ports = get_orig_ports(df, prefix)\n",
    "            neighbourhood_resp_ports = get_resp_ports(df, prefix)\n",
    "            neighbourhood_stats_means = get_stats_means(df, prefix)\n",
    "            neighbourhood_similar_count = get_similar_count(df, row, prefix)\n",
    "            neighbourhood_similar_attributes_count = get_similar_attributes_count(df, row, prefix)\n",
    "            \n",
    "            neighbourhood_dict.update(neighbourhood_counts)\n",
    "            neighbourhood_dict.update(neighbourhood_modes)\n",
    "            neighbourhood_dict.update(neighbourhood_means)\n",
    "            # neighbourhood_dict.update(neighbourhood_medians)\n",
    "            neighbourhood_dict.update(neighbourhood_orig_ports)\n",
    "            neighbourhood_dict.update(neighbourhood_resp_ports)\n",
    "            neighbourhood_dict.update(neighbourhood_stats_means)\n",
    "            neighbourhood_dict.update(neighbourhood_similar_count)\n",
    "            neighbourhood_dict.update(neighbourhood_similar_attributes_count)\n",
    "            \n",
    "            return neighbourhood_dict\n",
    "\n",
    "    return {prefix + '_total': 0,\n",
    "            prefix + '_proto_tcp_count': 0,\n",
    "            prefix + '_proto_udp_count': 0,\n",
    "            prefix + '_proto_icmp_count': 0,\n",
    "            prefix + '_connection.protocol_mode': '-',\n",
    "            prefix + '_connection.service_mode': '-',\n",
    "            prefix + '_connection.conn_state_mode': '-',\n",
    "            prefix + '_connection.time_mean': cur_time, # time_mean: 0 could not be here => problem later with time conversion (missing year) \n",
    "                                                        # (but does it make sense as a default value?)\n",
    "            prefix + '_connection.duration_mean': 0, \n",
    "            prefix + '_connection.orig_bytes_mean': 0,\n",
    "            prefix + '_connection.orig_pkts_mean': 0,\n",
    "            prefix + '_connection.resp_bytes_mean': 0,\n",
    "            prefix + '_connection.resp_pkts_mean': 0,\n",
    "            prefix + '_orig_p_well_known_count': 0,\n",
    "            prefix + '_orig_p_reg_or_dyn_count': 0,\n",
    "            prefix + '_resp_p_21_count': 0,\n",
    "            prefix + '_resp_p_22_count': 0,\n",
    "            prefix + '_resp_p_53_count': 0, \n",
    "            prefix + '_resp_p_80_count': 0,\n",
    "            prefix + '_resp_p_123_count': 0,\n",
    "            prefix + '_resp_p_443_count': 0,\n",
    "            prefix + '_resp_p_3389_count': 0,\n",
    "            prefix + '_resp_p_well_known_count': 0,\n",
    "            prefix + '_resp_p_reg_count': 0,\n",
    "            prefix + '_resp_p_dyn_count': 0,\n",
    "            prefix + '_dns_count_mean': 0,\n",
    "            prefix + '_ssh_count_mean': 0,\n",
    "            prefix + '_http_count_mean': 0,\n",
    "            prefix + '_ssl_count_mean': 0,\n",
    "            prefix + '_files_count_mean': 0,\n",
    "            prefix + '_similar_conns_count': 0,\n",
    "            prefix + '_similar_dns_qtype_count': 0,\n",
    "            prefix + '_similar_dns_rcode_count': 0,\n",
    "            prefix + '_similar_ssh_auth_attempts_count': 0,\n",
    "            prefix + '_similar_ssh_host_key_count': 0,\n",
    "            prefix + '_similar_http_method_count': 0,\n",
    "            prefix + '_similar_http_status_code_count': 0,\n",
    "            prefix + '_similar_http_user_agent_count': 0,\n",
    "            prefix + '_similar_ssl_version_count': 0,\n",
    "            prefix + '_similar_ssl_cipher_count': 0,\n",
    "            prefix + '_similar_ssl_curve_count': 0,\n",
    "            prefix + '_similar_ssl_validation_status_count': 0,\n",
    "            prefix + '_similar_files_source_count': 0,\n",
    "            prefix + '_similar_file_md5_count': 0\n",
    "           }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEIGHBOURHOOD_TIME_WINDOW_MINUTES_ORIG_DIRECTION = 5\n",
    "NEIGHBOURHOOD_TIME_WINDOW_MINUTES_RESP_DIRECTION = 2\n",
    "\n",
    "def compute_neighbourhoods(cur_orig_ip, dfs_list_orig, dfs_list_resp):\n",
    "    df_result = pd.DataFrame()\n",
    "    print('[{}]: Computing neighbourhood for connections of originator {:15} ({})'.format(datetime.now().strftime(\"%H:%M:%S\"), cur_orig_ip, str(len(dfs_list_orig[cur_orig_ip]))))\n",
    "    # iterate over rows in originated connections df of host with cur_orig_ip IP address:\n",
    "    for index, row in dfs_list_orig[cur_orig_ip].iterrows():\n",
    "        cur_row_dict = row.to_dict()\n",
    "        cur_time = row['connection.time']\n",
    "        \n",
    "        time_start_orig = cur_time - pd.Timedelta(minutes=NEIGHBOURHOOD_TIME_WINDOW_MINUTES_ORIG_DIRECTION)\n",
    "        time_end_orig = cur_time + pd.Timedelta(minutes=NEIGHBOURHOOD_TIME_WINDOW_MINUTES_ORIG_DIRECTION)\n",
    "        time_start_resp = cur_time - pd.Timedelta(minutes=NEIGHBOURHOOD_TIME_WINDOW_MINUTES_RESP_DIRECTION)\n",
    "        time_end_resp = cur_time + pd.Timedelta(minutes=NEIGHBOURHOOD_TIME_WINDOW_MINUTES_RESP_DIRECTION)\n",
    "        ip_responder = row['responded_ip']\n",
    "        try:\n",
    "            # compute neighbourhoods (from originated connections for originator, from responded connections for responder):\n",
    "            originator_neighbourhood = compute_time_neighbourhood(cur_orig_ip, dfs_list_orig, 'connection.time', cur_time, time_start_orig, time_end_orig, row, 'orig_orig')\n",
    "            originator_neighbourhood2 = compute_time_neighbourhood(cur_orig_ip, dfs_list_resp, 'connection.time', cur_time, time_start_resp, time_end_resp, row, 'orig_resp')\n",
    "            responder_neighbourhood = compute_time_neighbourhood(ip_responder, dfs_list_orig, 'connection.time', cur_time, time_start_orig, time_end_orig, row, 'resp_orig')\n",
    "            responder_neighbourhood2 = compute_time_neighbourhood(ip_responder, dfs_list_resp, 'connection.time', cur_time, time_start_resp, time_end_resp, row, 'resp_resp')\n",
    "\n",
    "            cur_row_dict.update(originator_neighbourhood)\n",
    "            cur_row_dict.update(originator_neighbourhood2)\n",
    "            cur_row_dict.update(responder_neighbourhood)\n",
    "            cur_row_dict.update(responder_neighbourhood2)\n",
    "            \n",
    "            # concat to one long row and to df_result:\n",
    "            row_df = pd.DataFrame([cur_row_dict])\n",
    "            df_result = pd.concat([df_result, row_df], axis=0, ignore_index=True)\n",
    "        except: \n",
    "            print('Problem with originator {} and responder {} ({})'.format(cur_orig_ip, ip_responder, row['connection.uid']))\n",
    "            pass\n",
    "    return df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start at 01:36:30.\n",
      "[01:36:33]: Computing neighbourhood for connections of originator 192.168.10.25   (242)\n",
      "[01:36:34]: Computing neighbourhood for connections of originator 192.168.10.8    (682)\n",
      "[01:36:35]: Computing neighbourhood for connections of originator 210.14.132.70   (1)\n",
      "[01:36:36]: Computing neighbourhood for connections of originator 192.168.10.51   (85)\n",
      "[01:36:37]: Computing neighbourhood for connections of originator 192.168.10.50   (313)\n",
      "[01:36:39]: Computing neighbourhood for connections of originator 192.168.10.12   (6182)\n",
      "[01:36:39]: Computing neighbourhood for connections of originator 192.168.10.17   (698)\n",
      "[01:36:41]: Computing neighbourhood for connections of originator 192.168.10.3    (3424)\n",
      "[01:36:42]: Computing neighbourhood for connections of originator 192.168.10.16   (1814)\n",
      "[01:36:45]: Computing neighbourhood for connections of originator 192.168.10.19   (269)\n",
      "[01:36:46]: Computing neighbourhood for connections of originator 192.168.10.9    (1489)\n",
      "[01:36:49]: Computing neighbourhood for connections of originator 192.168.10.15   (2777)\n",
      "[01:36:50]: Computing neighbourhood for connections of originator 192.168.10.14   (1596)\n",
      "[01:36:51]: Computing neighbourhood for connections of originator 192.168.10.5    (976)\n",
      "[01:36:53]: Computing neighbourhood for connections of originator 172.16.0.1      (95708)\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool\n",
    "from functools import partial\n",
    "from contextlib import contextmanager\n",
    "\n",
    "@contextmanager\n",
    "def poolcontext(*args, **kwargs):\n",
    "    pool = multiprocessing.Pool(*args, **kwargs)\n",
    "    yield pool\n",
    "    pool.terminate()\n",
    "\n",
    "# compute neighbourhoods using multiple threads (time optimalization):\n",
    "print('Start at ' + datetime.now().strftime(\"%H:%M:%S\") + '.')\n",
    "with poolcontext(processes=32) as pool:\n",
    "    \n",
    "    dfs_with_neighbourhoods = pool.map(\n",
    "        partial(compute_neighbourhoods, \n",
    "                dfs_list_orig=dfs_orig, \n",
    "                dfs_list_resp=dfs_resp), \n",
    "        dfs_orig.keys())\n",
    "\n",
    "print('Done at ' + datetime.now().strftime(\"%H:%M:%S\") + '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(dfs_with_neighbourhoods))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(dfs_with_neighbourhoods))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_with_neighbourhoods[0].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lYFgAiG_yjkY"
   },
   "source": [
    "### 2. Concatenate to one final DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "rjVpvSlvhtt_",
    "outputId": "0ba825ff-def0-4494-a513-8837fa99b265"
   },
   "outputs": [],
   "source": [
    "def concat_dfs(df_neighourhoods):\n",
    "    df_result = pd.DataFrame()\n",
    "    for i in range(0, len(df_neighourhoods)):\n",
    "        df_i = df_neighourhoods[i]\n",
    "        df_result = df_result.append(df_i)\n",
    "    return df_result\n",
    "\n",
    "df_result = concat_dfs(dfs_with_neighbourhoods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backup:\n",
    "\n",
    "from datetime import date\n",
    "\n",
    "df_result.to_csv(PREFIX + 'query_output_preprocessing_checkpoint_' + date.today().strftime(\"%d_%m\") + '.csv', index=False, header=True)\n",
    "\n",
    "# from datetime import datetime\n",
    "# import pandas as pd\n",
    "# df_result = pd.read_csv(PREFIX + 'query_output_preprocessing_checkpoint_' + date.today().strftime(\"%d_%m\") + '.csv')\n",
    "# df_result['connection.time'] = pd.to_datetime(df_result['connection.time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Assign attacker labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kS-tCgfhhtY9"
   },
   "outputs": [],
   "source": [
    "df_result['attacker_label'] = 'No'\n",
    "df_result['victim_label'] = 'No'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign labels to input data as:\n",
    "# 'No'  - not from/ to attacker\n",
    "# 'Yes' - originated from/ responded to attacker\n",
    "\n",
    "df_result.loc[df_result['responded_ip'] == '172.16.0.1', 'attacker_label'] = 'Yes'\n",
    "df_result.loc[df_result['originated_ip'] == '172.16.0.1', 'attacker_label'] = 'Yes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result['attacker_label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign labels to input data as:\n",
    "# 'No'  - not from/ to victim\n",
    "# 'Yes' - originated from/ responded to victim\n",
    "\n",
    "df_result.loc[df_result['responded_ip'] == '192.168.10.50', 'victim_label'] = 'Yes'\n",
    "df_result.loc[df_result['originated_ip'] == '192.168.10.50', 'victim_label'] = 'Yes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result['victim_label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Write to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result.to_csv(PREFIX + 'query_output_processing.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df_result.columns:\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "concat_connection_csv_files.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (base)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
